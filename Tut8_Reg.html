<!DOCTYPE html>
<html lang="en">
    <head>
        
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SPPQKTW0JB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SPPQKTW0JB');
</script>

          
<script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=6617877f27bd20001a32957e&product=inline-share-buttons&source=platform" async="async"></script>
        
        <!--        Google ADSENSE-->
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8357043682630159"
     crossorigin="anonymous"></script>
       
             <!--MEDIA-->
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <style>
        .column {
  width: 100%;
}

@media (min-width: 600px) {
  .column {
    width: 50%;
  }
}
        </style>
         <meta name="description" content="Tutorial on caluation of loss or error functions and R square for regression analysis in machine learning." />
        <meta name="author" content="Joyita Bhattacharya" />
        <meta name="keywords" content="Loss functions, Goodness of fit, R-squared, mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), thermolectric, kappa, sigma, Seebeck, ZT, evaluation metrics, calculate, compute, input features, Prediction"/>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v15.0" nonce="A5brOO1e"></script>
        
        <!-- Navigation-->
        
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                   Content
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                        
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="blog_list.html">Blogs</a>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="tutorial_list.html">Tutorials</a></li>
                        
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        
        <header class="home_header" style="background-image: linear-gradient(to left,black,teal, yellow,orange)">
            <meta charset="utf-8" />
            
      <top><img src ="assets/img/logo_mdxp.png" width=5%></top>
    

            <div class="container position-relative px-2 px-lg-2">
                <div class="row gx-4 gx-lg-5 justify-content-left">
                    <div class="col-xs-15 col-md-10 col-lg-8 col-xl-7">
                        <meta name="viewport" content="width=device-width, initial-scale=1" />
                            </div></div></div>                 
                         </header>
            
                    
<!--Title of the blog-->
                    <p><h1 align="center"><font face ="verdana", color = "maroon">Assessing Evaluation Metrics for  Regression Analysis with a Single Input Variable (Part I)    </font></h1>
                    <center><p> <font face="monospace"> February, 2024</font></p></center>                                          
                    <hr size =4 >
                            
                                   
                        <!--CONTENT-->
                                                                                                                     
              <div class="container position-relative px-2 px-lg-2">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-xs-15 col-md-10 col-lg-8 col-xl-7">
                        <p>The main goal of <a href='https://en.wikipedia.org/wiki/Linear_regression'><b><font color=blue>regression analysis</font></b></a> is to build a model that accurately predicts the target variable based on the input features. However, the accuracy of the model can only be determined by comparing its predicted values to the actual target values.</p>
                    <p>Several evaluation metrics can be used in regression analysis, such as <b>mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE),</b> and <b>R<sup>2</sup></b>. While <b>MSE</b>, <b>RMSE</b>, and <b>MAE</b> are the <b>error functions</b>, <b>R<sup>2</sup></b>, also known as goodness of fit or coefficient of determination, measures the <b>model’s performance</b>. The mathematical equations of these evaluation metrics are tabulated for understanding and convenience.</p>
                    
                    <p><center><img src='assets/Tut8_9_Reg/err_Rsq1.jpg' width="100%"><span class="caption text-muted" >The evaluation metrics for regression analysis.</span></center></p>
                    <p><i>In general, a good regression model should have low values for MSE, RMSE, and MAE, indicating that the predicted values are close to the actual values. A high R<sup>2</sup> score indicates that the model can explain a large proportion of the variance in the data.</i></p> 
<p><mark style="background-color: antiquewhite">The choice of metric depends on the specific problem and the desired outcome. That being said, I will <b>demonstrate how the number (quantity) and quality (i.e., how essential they are to the response ) of input features</b> impact the model fitting and prediction, and consequently the evaluation metrics in two separate tutorials.  
</mark></p>
                        <p><b>A brief technical note on the working dataset</b></p>
The dataset comprises 204 thermoelectric materials (compositions), their room temperature transport properties, and the performance metrics, ZT. 
                        <center> <img src="assets/Tut8_9_Reg/IO.jpg", width=100%></center>
<p>Note that two features (S<sup>2</sup> and sigma/kappa) are derived, and they play a major role in the evaluation of ZT.</p>
                        <p> ZT is directly proportional to S<sup>2</sup>, sigma (σ), and holds an inverse relationship with kappa (κ). 
 Here is the equation connecting ZT with the transport properties. 
</p>
                        <p><center><img src='assets/Tut8_9_Reg/eqZT.jpg' width="80%"></center></p>
                        <p>Let’s now go through the coding steps to compute the error functions and performance metrics.</p>
                
                        <h4>Simple Linear Regression Analysis with a Single Input Feature</h4>
                        <p><u>Coding for Computing Error/Loss functions and performance metric </u></p>
                    <p><b>Step 1: Importing the relevant libraries and loading the dataset</b></p>
                        
                                            <pre style="background-color: gainsboro"><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# for machine learning
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# for loss functions and R square
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from  sklearn.metrics import r2_score</code></pre>
                        <p><b>Input feature considered:</b> kappa; <b>Target:</b> ZT</p>
                         <pre style="background-color: gainsboro"><code>dataset = pd.read_csv('RT_TEprop.csv')
X = dataset.iloc[:, 4].values
y = dataset.iloc[:, -1].values</code></pre>
                        <p><b>Step 2: Splitting data into training and test sets</b></p>
                        <pre style="background-color: gainsboro"><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)</code></pre>
                        <pre style="background-color: gainsboro"><code># Reshaping required as fit() method of the LinearRegression class expects a 2D array as input.
X_train=X_train.reshape(183,1)
y_train=y_train.reshape(183,1)</code></pre>
                     <p><b>Step 3: Fitting linear regression model to the training set</b></p>
                             <pre style="background-color: gainsboro"><code>lr = LinearRegression()
lr.fit(X_train, y_train)</code></pre>
                        
                        
                        <p><b>Step 4: Prediction</b></p>
   
                                     
                    <pre style="background-color: gainsboro"><code>X_test=X_test.reshape(21,1)
y_test=y_test.reshape(21,1) 
y_pred = lr.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))</code></pre>
                                           
                        <p><b> Plotting</b></p>
                        
                                        <pre style="background-color: gainsboro"><code>plt.scatter(X_test, y_test)
plt.plot(X_train, lr.predict(X_train), "r")
plt.title("kappa vs ZT (Test set)")
plt.xlabel("kappa")
plt.ylabel("ZT")
plt.show()</code></pre>
                        <center><img src="assets/Tut8_9_Reg/LinReg_plot.jpg" width=80%></center>
                        
<!--      ERRORS & Rsquared-->
                        
                        <p><b>Step 5: Computing error/ loss functions such as MAE, MSE, RMSE</b></p>
                        
                        <pre style="background-color: gainsboro"><code>print("MAE",mean_absolute_error(y_test,y_pred))
                    </code></pre>
                        <pre style="background-color: greenyellow"><code>MAE 0.07059993216927259
                    </code></pre>
                       
                        <pre style="background-color: gainsboro"><code>print("Mean square error is ",mean_squared_error(y_test,y_pred))
                    </code></pre>
                        <pre style="background-color: greenyellow"><code>Mean square error is  0.00606286753428833
                    </code></pre>
                         
                        <pre style="background-color: gainsboro"><code>rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred))
rmse_lr
                    </code></pre>
                        <pre style="background-color: greenyellow"><code>0.07786441763917798
                    </code></pre>
                        
<p><b>Step 6: Computing R2</b></p>
                        
                        <pre style="background-color: gainsboro"><code>r2_score_lr= r2_score(y_test, y_pred)
r2_score_lr
         </code></pre>
                        <pre style="background-color: greenyellow"><code>-0.1281233534684132
                    </code></pre>
                    
                        <p><i>A negative goodness of fit in machine learning suggests that the model performs worse than a simple baseline model. This indicates that the model needs improvement or a different modeling approach to better capture the underlying patterns in the data.</i></p>
                        <p><i> Therefore, we can <b>apply polynomial regression fitting</b> to see if there is any improvement in the model's performance. </i></p>
                        
                        <h4>Polynomial Linear Regression Analysis with Single Input Feature</h4>
                        <p>We now fit a polynomial regression model to the training dataset using the same input variable, kappa to observe any improvement in the R<sup>2</sup> value. We will be using the same dataset as before. So, let's directly move to the polynomial regression part.</p>
                        <p><b>Import <a href='https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html'><b><font color=blue>PolynomialFeatures</font></b></a> from sklearn.preprocessing class</b> for polynomial regression fitting. <b>'PolynomialFeatures' generates a new feature matrix</b> of polynomial combinations with specified degrees followed by applying the fit_tranform() method to the input variable. </p>
                        
              <pre style="background-color: gainsboro"><code> poly = PolynomialFeatures(degree=2)
poly_features_1 = poly.fit_transform(X.reshape(-1,1))
X1_train, X1_test, y1_train, y1test = train_test_split(poly_features_1, y, test_size=0.1, random_state=42)</code></pre>
                        
                        <pre style="background-color: gainsboro"><code>poly_features_1.shape</code></pre>
                        <pre style="background-color: greenyellow"><code></code>(204, 3)</pre></code>
                    
                        <p><mark style="background-color: blanchedalmond">Note: New matrix has 204 rows, 3 columns for kappa values. Original data: 204 rows, 1 column. </mark></p>                   

                    <center><img src="assets/Tut8_9_Reg/3cols.jpg" width=80%></center>
                    
                    <p><pre style="background-color: gainsboro"><code>poly_features_1</code></pre></p>
                    <p>A snapshot of the output, displayig the new matrix, is shown below.</p>
                        
                        <p><pre style="background-color: greenyellow"><code>array([[1.00e+00, 2.51e+00, 6.32e+00],
       [1.00e+00, 2.27e+00, 5.14e+00],
       [1.00e+00, 2.50e+00, 6.25e+00],
       [1.00e+00, 1.79e+00, 3.19e+00],
       [1.00e+00, 2.85e+00, 8.12e+00],
       [1.00e+00, 1.48e+00, 2.20e+00],
       [1.00e+00, 2.17e+00, 4.71e+00],
       [1.00e+00, 1.64e+00, 2.68e+00],
       [1.00e+00, 2.14e+00, 4.60e+00],
       [1.00e+00, 1.49e+00, 2.22e+00],
       [1.00e+00, 1.79e+00, 3.21e+00],
       [1.00e+00, 3.56e+00, 1.27e+01],</code></pre>
                      
                    <p><b>Splitting the data into training and test sets</b><p>
                    <p>The training dataset consists of 183 rows and 3 columns against 1 column in the original dataset used for simple regression analysis.</p> 
                    
                    
<pre style="background-color: gainsboro"><code>X1_train, X1_test, y1_train, y1test = train_test_split(poly_features_1, y, test_size=0.1, random_state=42)
</code></pre>
                    
                    <p>After splitting the data, <i>same steps are followed as before such as linear regression fitting of the training data, prediction using the test set followed by computation of loss functions and R<sup>2</sup></i>. The values of these parameters are tabulated with the help of tabulate function of Python <a href="https://pypi.org/project/tabulate/"><font color =blue><b>tabulate module</b></font></a>.</p>
                    <p>The coding and the output for the same are shown below. </p> 
                        <pre style="background-color: gainsboro"><code>from tabulate import tabulate

Eval_table = [["MAE", 0.089], ["MSE",0.016], ['RMSE', 0.125], ['R-squared', -0.015]]
print(tabulate(Eval_table)) </code></pre>    
                   
<center><pre style="background-color: greenyellow"><code>---------  ------
MAE         0.089
MSE         0.016
RMSE        0.125
R-squared  -0.015
---------  ------</code></pre></center>
                  <p>We found that both simple and polynomial linear regression fittings result in similar outcomes with respect to loss functions and R<sup>2</sup>.</p>
                        <p><mark style="background-color: antiquewhite">In the next tutorial, we will delve into the <b>impact of incorporating additional input variables and selecting appropriate types of input variables </b>on the evaluation metrics of the machine learning model, aiming for improved prediction outcomes.</mark></p>
     
                  <p><i> <a href="https://github.com/bjoyita/Regression_ML/blob/main/RegTut_part1.ipynb"><b><font color=blue>Link</font></b></a> to the entire code and dataset.</i></p>
 <!-- Footer-->
        <footer class="border-top">
      
                      <!-- ShareThis BEGIN -->
                        <p>
                <div class="sharethis-inline-share-buttons"></div> 
<!-- ShareThis END -->
            <!--!--SHARE BUTTON -->
<div>
                       <div class="container position-relative px-2 px-lg-2">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-xs-15 col-md-10 col-lg-8 col-xl-7">
<!--
                        <script src="https://static.elfsight.com/platform/platform.js" data-use-service-core defer></script>
                        <div class="elfsight-app-76277869-2ec6-4fa9-a07b-1c0a871e383f" data-elfsight-app-lazy></div></div></div></div>
-->
                       <p>
                        <div class="small text-center text-muted">
                            <p><i>&#169 2024 Materials Data Explorer by Joyita Bhattacharya. All Rights Reserved.</i></p>
                            <p><a href='https://www.iubenda.com/privacy-policy/28979278/full-legal'>Privacy policy</a> |<a href='https://www.iubenda.com/privacy-policy/28979278/cookie-policy'> Cookie policy</a> | <a href='https://www.iubenda.com/terms-and-conditions/28979278'>  Terms &#38 Conditions </a></p></div></div></div></div>
                            
<!--                            <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a href='PrivacyPolicy.html'>Privacy policy</a>| <a property="dct:title" rel="cc:attributionURL" href="https://bjoyita.github.io/">Materials Data Explorer</a> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://www.linkedin.com/in/joyita-de-bhattacharya-6b949b1b">Joyita Bhattacharya</a> is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p> </div>-->
                    </div>
            
                  </footer></div>
 <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
            </body></html>
                        
                        
                        
                        
                        
